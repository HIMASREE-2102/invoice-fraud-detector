# -*- coding: utf-8 -*-
"""fraud_detection_evaluation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNg9kVulbhd2MfeYYNsdyZfeNwkCEBXi
"""

# ==========================================================
# ðŸ“Š Fraud Detection Evaluation (with Auto Simulation)
# ==========================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    accuracy_score, precision_score, recall_score, f1_score
)
from google.colab import files

# -------------------- Step 1: Upload CSV --------------------
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)

print("âœ… File loaded:", file_name)
print("Shape:", df.shape)
print("\nColumns:", df.columns.tolist())
display(df.head())

# -------------------- Step 2: Data Cleaning --------------------
if 'Status' not in df.columns:
    raise ValueError("âŒ 'Status' column missing! Please check the CSV.")

df['Status'] = df['Status'].astype(str).str.strip()
df['Status_bin'] = df['Status'].str.lower().map({'fraud': 1, 'legit': 0})
df = df.dropna(subset=['Status_bin'])

# -------------------- Step 3: Simulate Predictions (if missing) --------------------
# Simulate realistic model outputs if missing
if 'rf_pred' not in df.columns:
    df['rf_pred'] = np.where(df['Amount'] > df['Amount'].median(), 1, 0)
    print("ðŸ§  Simulated Random Forest predictions added.")

if 'xgb_pred' not in df.columns:
    df['xgb_pred'] = np.where((df['Tax'] > 10) | (df['Payment_Terms'] == 'Net7'), 1, 0)
    print("ðŸ§  Simulated XGBoost predictions added.")

if 'ae_pred' not in df.columns:
    df['ae_pred'] = np.random.choice([0, 1], size=len(df), p=[0.9, 0.1])
    print("ðŸ§  Simulated Autoencoder predictions added.")

# -------------------- Step 4: Model Evaluation Function --------------------
def evaluate_model(df, model_col, model_name):
    """Evaluate model predictions and return metrics"""
    y_true = df.loc[df[model_col].notna(), 'Status_bin']
    y_pred = df.loc[df[model_col].notna(), model_col]

    print(f"\nðŸ“Š {model_name} Results:")
    cm = confusion_matrix(y_true, y_pred)
    print(cm)
    print(classification_report(y_true, y_pred, digits=3))

    # Compute metrics
    metrics = {
        "Model": model_name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, zero_division=0),
        "Recall": recall_score(y_true, y_pred, zero_division=0),
        "F1 Score": f1_score(y_true, y_pred, zero_division=0),
        "ROC AUC": roc_auc_score(y_true, y_pred)
    }

    # Plot confusion matrix
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"{model_name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    return metrics

# -------------------- Step 5: Evaluate All Models --------------------
metrics_list = []
for col, name in zip(['rf_pred', 'xgb_pred', 'ae_pred'],
                     ['Random Forest', 'XGBoost', 'Autoencoder']):
    metrics_list.append(evaluate_model(df, col, name))

# -------------------- Step 6: Show Overall Summary --------------------
summary_df = pd.DataFrame(metrics_list)
print("\nðŸ“ˆ Overall Performance Summary:")
display(summary_df.style.background_gradient(cmap='YlGnBu'))

# Bar chart comparison
summary_df_melt = summary_df.melt(id_vars='Model', var_name='Metric', value_name='Score')
plt.figure(figsize=(8, 5))
sns.barplot(data=summary_df_melt, x='Metric', y='Score', hue='Model')
plt.title("Model Performance Comparison")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# -------------------- Step 7: Fraud Detection Summary --------------------
fraud_counts = df['Status'].value_counts()
summary_counts = {
    'Actual Fraud Count': int(fraud_counts.get('Fraud', 0)),
    'RF Predicted Fraud': int(df['rf_pred'].sum()),
    'XGB Predicted Fraud': int(df['xgb_pred'].sum()),
    'AE Predicted Fraud': int(df['ae_pred'].sum())
}

print("\n==================== Detection Summary ====================")
for k, v in summary_counts.items():
    print(f"{k}: {v}")
print("===========================================================")

# -------------------- Step 8: Export Results --------------------
summary_df.to_csv("model_performance_summary.csv", index=False)
df.to_csv("invoices_with_predictions.csv", index=False)
print("\nðŸ’¾ Exported:")
print(" - model_performance_summary.csv")
print(" - invoices_with_predictions.csv")

files.download("model_performance_summary.csv")
files.download("invoices_with_predictions.csv")